---
title: "Machine Learning in R"
author: "Nick Gabriele"
date: "11/22/2019"
output: 
  html_document:
    toc: true
    toc_float: true
    number_sections: true
    theme: cerulean
---

```{r setup, include=F}
knitr::opts_chunk$set(echo = T)
```

# Introduction

## A Brief Recap

### Data

The Singapore automobile claim dataset is revisited in the context of basic machine learning models. The advantages of using this dataset are that it (1) is simple to understand, (2) requires no extensive preprocessing, and (3) conveys some basic insurance and machine learning concepts.

Recall that the original dataset consists of 7,483 observations and the following columns:

* Target variable: $Clm\_Count$, the number of claims incurred.
* Predictor variables:
  + $SexInsured$, a category for the gender of the policyholder($M =$ male, $F =$ female, $U =$ unknown).
  + $Female$, an indicator for whether the policyholder is female (1 if affirmative).
  + $VehicleType$, the type of vehicle covered ($A =$ automobile, $T =$ truck, $M =$ motorcycle, etc).
  + $PC$, an indicator for whether the car is privately owned (1 if affirmative).
  + $Exp\_weights$, the policy-year exposure weight.
  + $LNWEIGHT$, the logarithm of exposure weight.
  + $NCD$, the percentage of premium discounted for policies with no claims.
  + $AgeCat$, a category for the age of the policyholder, binned as follows:

    | Category| Age of Policyholder|
    |--------:|-------------------:|
    |        0|           $[0, 22)$|
    |        1|          $[22, 26)$|
    |        2|          $[26, 36)$|
    |        3|          $[36, 46)$|
    |        4|          $[46, 56)$|
    |        5|          $[56, 66)$|
    |        6|      $[66, \infty)$|

  + $VAgeCat$, a category for the age of the vehicle, binned as follows:

    | Category|      Age of Vehicle|
    |--------:|-------------------:|
    |        0|            $[0, 1)$|
    |        1|            $[1, 2)$|
    |        2|            $[2, 3)$|
    |        3|            $[3, 6)$|
    |        4|           $[6, 11)$|
    |        5|          $[11, 16)$|
    |        6|      $[16, \infty)$|

### Notes

A few notes for those unfamiliar with insurance datasets:

* __Exposure__ refers to the relative weight of an observation, often based on time, count, dollar amount, or some other measure. It is usually proportional to the underlying risk. The longer an insurance policy has been in-force, the higher the exposure. The higher the exposure, the more _statistically credible_ it is as a consequence of the longer observation window.
  + The natural logarithm of exposure, $LNWEIGHT$, is useful when claim frequency follows a Poisson distribution. In __ratesetting__ studies, a special property allows this quantity to be used to adjust the base claim rate as an additive constant to account for exposure.
* Several types of insurance policies offer __experience refunds__ that return or discount a portion of premiums as a reward for favorable claim activity (from the perspective of the insurance company). This is represented by the $NCD$ column, which is the percentage discount offered based on favorable experience.

> __Knowledge Check:__ Suppose you are comparing two observations, $i$ and $j$, where ${NCD}_i = 20$ and ${NCD}_j = 50$. For which policy would you expect to see a higher $Clm\_Count$, all else equal?

## Model Setup

### Loading the Data

Load the original dataset as a dataframe.

```{r}
library(insuranceData)
data(SingaporeAuto)  # Load dataset into environment
data.all <- as.data.frame(SingaporeAuto)  
#data.all[, c('AutoAge0', 'AutoAge1', 'AutoAge2', 'AutoAge')] <- NULL  # Remove interactions
head(data.all)  # Display a few rows
```

### Reclassifying the Response

To simplify the problem, we recode the target variable to indicate _whether_ a policy has incurred claims (1 if affirmative), as opposed to claim frequency. This allows us to consider classification methods, an indispensible class of machine learning models. Such an action is also justified because instances of two or more claims are sparse and therefore lack credibility.

```{r}
data.all$Clm_Flag <- as.factor(ifelse(data.all$Clm_Count >= 1, "Y", "N"))  # Recode to binary classification
data.all$Clm_Count <- NULL
summary(data.all$Clm_Flag)  # Show number of instances where claims occurred
```

### Dividing the Data

Split the dataframe into training/testing subsets.

```{r message=F}
library(caret)
set.seed(1000)  # Set seed for reproducibility
training.indices <- createDataPartition(data.all$Clm_Flag, p = 0.8, list = F)  # Partition data
data.training <- data.all[training.indices, ]  # Use selected rows for training data
data.testing <- data.all[-training.indices, ]  # Use remainder for testing data
```

For insurance datasets involving claims, it is common for the positive (i.e. $Clm\_Flag =$ "Y") responses to represent a minority of the overall exposures. This can lead to weak discriminating power between the positive and negative classes. One possible solution is to employ oversampling of the minority group, undersampling of the majority group, or some combination thereof. This can enhance the definition of the borders of each class in the feature space, and thus improve sensitivity (minimize false negatives) and precision (minimize false positives).

```{r message=F}
samp.scale <- sum(data.training[data.training$Clm_Flag == "N", "Exp_weights"]) /
  sum(data.training[data.training$Clm_Flag == "Y", "Exp_weights"])
data.training$Exp_weights <- data.training$Exp_weights * ifelse(data.training$Clm_Flag == "Y", samp.scale, 1)

library(dplyr)
data.training %>%
  group_by(data.training$Clm_Flag) %>%
  summarize(Count = n(), Exposure = sum(Exp_weights), AvgExposure = n()/sum(Exp_weights))

```

> __Knowledge Check:__ In the above code, oversampling is performed without adding any new rows. How is this possible?

# Non-Parametric Modeling Techniques

## Decision Trees

### Building a Tree

A decision tree recursively partitions the feature space by selecting a split point that maximizes the impurity reduction (for classification) or minimizes the loss function (for regression) at each node. The algorithm terminates when one of the control parameters (e.g. maximum depth, minimum bucket size) has been invalidated or no further improvement can be made. The resulting leaf nodes are the basis for new predictions.

```{r}
library(rpart)
library(rpart.plot)
model.tree <- rpart(Clm_Flag ~ SexInsured + Female + VehicleType + PC + NCD + AgeCat + VAgeCat,
                    data = data.training,
                    weights = Exp_weights,  # Use exposure weights
                    method = "class",
                    control = rpart.control(minsplit = 10,  # Specify hyperparameters
                                            minbucket = 1,
                                            cp = 0.005,
                                            maxdepth = 5,
                                            xval = 5),
                    parms = list(split = "gini"))  # Use Gini measure of impurity

rpart.plot(model.tree)  # Visualize the tree model
```

> __Knowledge Check:__ The above tree is color-coded; green if $Clm\_Flag =$ "Y" and blue otherwise. However, the degree of _shading_ also varies between nodes. How does this relate to the concept of impurity?

### Using Cross-Validation

Decision trees are broadly applicable, straightforward to interpret, and robust against missing values (via __surrogate splits__), but are vulnerable to _overfitting_ and tend to favor highly divisible features. However, this can be addressed by:

* Cost-complexity _pruning_, a post-processing step that removes selected nodes based on impurity criteria.
* __Cross-validation__, which evenly divides the dataset into $n$ folds, each of which contains a holdout set to evaluate a model that has been trained on the other $n-1$ folds.

In the latter case, any of the __hyperparameters__ can be _tuned_ against the average error rate. In the interest of parsimony, an appropriate selection of a hyperparameter usually considers the point at which there is diminishing benefit for cross-validation error.

```{r}
plotcp(model.tree)  # Plot cross-validation error versus complexity parameter
```

While cross-validation is a reliable technique in the machine learning toolkit, it is important to remember that this it is only as useful as the available data, and there may be unobservable factors governing the true population dynamics. In particular, if the dataset is sparsely populated or lacking in distinctive features (a.k.a. __signal__), it is still possible to overfit to noise. There may also be cases where it is not appropriate to partition (e.g. imbalanced datasets result in degenerative folds) or cases where the partitioned sets underfit the data, but this should be detectable across folds.

> __Knowledge Check:__ The above graph plots the average error rate produced over the $n$ folds for possible choices of the _complexity parameter_, defined as the minimum impurity reduction needed for the learning algorithm to continue splitting nodes. What is a reasonable choice for the value of the complexity parameter, and how would the depth of the tree change?

### Predicting New Observations

A predictive modeling problem typically involves taking a position on what might happen in the future based on the historical data, in contrast to descriptive or prescriptive analytics. To assess whether the model generalizes well to as-yet unseen data, the testing set is utilized. By holding out a portion of data until the evaluation stage, models can be fairly compared against each other in what approximates a live setting.

```{r}
pred.tree <- predict(model.tree, data.testing, type = "prob")  # Generate predicted claim incidence
pred.tree <- pred.tree[, "Y"] * data.testing$Exp_weights  # Multiply by policy year exposure
pred.tree <- as.factor(ifelse(pred.tree <= 0.5, "N", "Y"))  # Map to binary response using threshold

library(e1071)
confusionMatrix(data = pred.tree, reference = data.testing$Clm_Flag, positive = "Y")  # Create confusion matrix
```

For reference, the performance measures printed in the confusion matrix are formulated as follows:

|              Performance Measure|                                                                    Formula|
|--------------------------------:|--------------------------------------------------------------------------:|
|                  Accuracy, $Acc$|                                        $\frac{TP + TN}{TP + TN + FP + FN}$|
|       No Information Rate, $NIR$|                                        $\frac{TN + FP}{TP + TN + FP + FN}$|
|                 Cohen's $\kappa$|   $1 - \frac{1 - Acc}{1 - (1 - NIR) \times PPCR - ( 1 - PPCR) \times NIR}$|
|                   McNemar's Test| $\chi_1^2 = \frac{{\left( \left| FP - FN \right| - 1 \right)}^2}{FP + FN}$|
|               Sensitivity, $TPR$|                                                       $\frac{TP}{TP + FN}$|
|               Specificity, $TNR$|                                                       $\frac{TN}{TN + FP}$|
| Positive Predictive Value, $PPV$|                                                       $\frac{TP}{TP + FP}$|
| Negative Predictive Value, $NPV$|                                                       $\frac{TN}{TN + FN}$|
|              Prevalence, $1-NIR$|                                        $\frac{TP + FN}{TP + TN + FP + FN}$|
|                   Detection Rate|                                             $\frac{TP}{TP + TN + FP + FN}$|
|     Detection Prevalence, $PPCR$|                                        $\frac{TP + FP}{TP + TN + FP + FN}$|
|                Balanced Accuracy|                                     $\frac{1}{2} \left( TPR + TNR \right)$|

> __Knowledge Check:__ The predictions above are retrieved as claim _incidence_ rates using the argument `type = "prob"` as opposed to `"class"`, which would make "Y" or "N" predictions. Why is it more appropriate to use `"prob"` even though this is a classification problem?

## Bagging

### Bootstrapping Observations

Leveraging the wisdom of crowds, ensemble methods construct several submodels to extract complex signals from the data, which are then aggregated in some way so as to retain generality. One way to invoke this is by resampling the training data to build a parallel series of models in a procedure known as __bagging__. A voting mechanism or aggregation process is needed, often weighted by the underlying models' performance, to render a final prediction.

```{r message=F}
library(ranger)
set.seed(1000)  # Set seed for reproducibility
model.bag <- ranger(Clm_Flag ~ SexInsured + Female + VehicleType + PC + NCD + AgeCat + VAgeCat,
                    data = data.training,
                    num.trees = 20,  # Number of trees to construct
                    mtry = 7,
                    importance = 'impurity',
                    probability = T,
                    min.node.size = 10,
                    max.depth = 5,
                    replace = T,
                    sample.fraction = 0.75, # Number of observations for each tree
                    case.weights = data.training$Exp_weights,  # Use exposure weights
                    splitrule = 'gini')

MapPrediction <- function(model, data, threshold) {
  predictions <- predict(model, data)  # Generate predicted claim incidence
  predictions <- predictions$predictions[, "Y"] * data$Exp_weights  # Multiply by policy year exposure
  predictions <- as.factor(ifelse(predictions <= threshold, "N", "Y"))  # Map to binary response using threshold
  predictions
}

pred.bag <- MapPrediction(model.bag, data.testing, 0.5)
confusionMatrix(data = pred.bag, reference = data.testing$Clm_Flag, positive = "Y")
```

> __Knowledge Check:__ Examine the arguments of the ``MapPredictions`` function. How do the performance measures change when the function evaluates the training data instead of the testing data? What happens to $TPR$ and $TNR$ as the probability threshold increases beyond 0.5?

### Ranking Variable Importance

A common criticism of ensemble methods is that their oft-superior predictive capabilities come at the expense of transparency. Whereas linear models can effortlessly link two or more variables via coefficients, non-parametric models are inherently fragmented and abstractions are often strained. However, this can be mitigated with feature importance rankings. In particular, the number of times a feature is used to partition across trees can be enumerated, and weighted in proportion to the number of observations it splits.

```{r}
var.imp <- data.frame(Feature = names(model.bag$variable.importance),  # Extract variable importance rankings
                      Splits = model.bag$variable.importance, row.names = NULL)

library(ggplot2)
ggplot(var.imp, aes(x = reorder(Feature, Splits), y = Splits)) +  # Plot variable importance
  geom_bar(stat = 'identity') +
  labs(x = "Feature", y = "Number of Times Feature is Used to Partition") +
  coord_flip()
```

> __Knowledge Check:__ How is it possible for the variables $VAgeCat$ and $NCD$ to be used 106 and 68 times, respectively, when only 50 trees are constructed?

## Random Forests

### Bootstrapping Features

Decision trees are often described as _greedy_ algorithms that make sequential decisions without revisiting prior steps, meaning that the desire to minimize impurity at an early stage may come at the expense of discovering important relationships at a later stage. To circumvent this, a __random forest__ builds on the previous concept of bootstrap aggregation by randomly sampling a "bag" of features from which to construct a single tree. The process is then repeated until a collection of trees is established, through which predictions are made via a consensus algorithm.

```{r}
library(ranger)
set.seed(1000)
model.forest <- ranger(Clm_Flag ~ SexInsured + Female + VehicleType + PC + NCD + AgeCat + VAgeCat,
                       data = data.training,
                       num.trees = 20, 
                       mtry = 3, # Number of features for each tree
                       probability = T,
                       min.node.size = 10,
                       max.depth = 5,
                       replace = T,
                       sample.fraction = 0.75,
                       case.weights = data.training$Exp_weights,
                       splitrule = 'gini')

pred.forest <- MapPrediction(model.forest, data.testing, 0.5)
confusionMatrix(data = pred.forest, reference = data.testing$Clm_Flag, positive = "Y")
```

> __Knowledge Check:__ Experiment with the different hyperparameter values in the random forest model for the training data. Are you able to improve the model fit? Do the same gains carry over once evaluated against the testing data?

### Tuning Hyperparameters

The structure of the model implicitly includes certain hyperparameters, such as ``mtry``, the number of variables selected or ``max.depth``, the maximum depth of a tree model. It is possible to construct a __tuning grid__, consisting of various combinations of hyperparameters, over which the learning algorithm searches for the best calibration according to some performance measure (e.g. $1-ACC$, the error rate). The reasoning is that if the model structure performs reliably well on the training set, it should generalize to a testing set.

```{r}
set.seed(1000)

hyper.grid <- expand.grid(num.trees = 1:20,
                          mtry = 2:5,
                          sample.fraction = c(0.2, 0.4, 0.6, 0.8, 1.0),
                          train.error = 0,
                          test.error = 0)

for(i in 1:nrow(hyper.grid)){
  model.tuned <- ranger(Clm_Flag ~ SexInsured + Female + VehicleType + PC + NCD + AgeCat + VAgeCat,
                        data = data.training,
                        num.trees = hyper.grid$num.trees[i],
                        mtry = hyper.grid$mtry[i],
                        sample.fraction = hyper.grid$sample.fraction[i],
                        probability = T,
                        min.node.size = 10,
                        max.depth = 5,
                        replace = T,
                        case.weights = data.training$Exp_weights,
                        splitrule = 'gini')
  pred.tuned1 <- MapPrediction(model.tuned, data.training, 0.5)
  hyper.grid$train.error[i] <- 1 - confusionMatrix(data = pred.tuned1,
                                                   reference = data.training$Clm_Flag,
                                                   positive = "Y")$overall[[1]]
  pred.tuned2 <- MapPrediction(model.tuned, data.testing, 0.5)
  hyper.grid$test.error[i] <- 1 - confusionMatrix(data = pred.tuned2,
                                                  reference = data.testing$Clm_Flag,
                                                  positive = "Y")$overall[[1]]
}

hyper.grid[order(hyper.grid$train.error)[1:5],]  # Display instances of minimum training error
```

To preserve the integrity of the testing, it is important that any decisions involving feature selection or hyperparameter tuning are made independently of the unseen data to prevent a phenomenon known as _data leakage_.

> __Knowledge Check:__ Alter the tuning grid to include ``max.depth`` evaluated over the domain ``c(2, 5, 10)``. What combination of hyperparameter values minimizes overall training error?

### Addressing the Bias-Variance Tradeoff

There is a natural tendency to introduce complexity into a model for the sake of increasing predictive accuracy, but when exposed to new data this can result in inconsistent errors (i.e. variance), owing to the inclusion of irrelevant features. On the other hand, overly simplified model structures tend to generate consistent errors (i.e. bias), owing to the exclusion of relevant features.

Collectively, this phenomenon is referred to as the __bias-variance tradeoff__, which acknowledges that expected loss, while not being eliminable, can at least be minimized within some interior of these effects. A plot of training versus testing error illustrates how finely-tuned models often struggle when applied to a new dataset. 

```{r}
ggplot(subset(hyper.grid, mtry == 5 & sample.fraction == 0.4)) +
  geom_line(aes(y = test.error, x = num.trees, color = "test"), size = 1.5) +
  geom_line(aes(y = train.error, x = num.trees, color = "train"), size = 1.5) +
  theme(legend.position="bottom", legend.direction="horizontal", legend.title = element_blank()) +
  labs(x = "Number of Trees", y = "Error Rate", title = "Random Forest with 5 of 7 Features, 40% Resampling") +
  scale_x_continuous(breaks = seq(0, 20, 2)) +
  scale_color_manual(labels = c("Testing Error", "Training Error"),
                     values = c('test' = "orange", 'train' = "blue"))
```

> __Knowledge Check:__ What causes the error rate of the testing set higher than that of the training set?

# Diagnostic Tools

## Receiver Operating Characteristics

In binary classification problems, a confusion matrix tabulates the number of times that the predicted outcome correctly or incorrectly matched the positive or negative classes. A perfect model maximizes positive predictions among the positive observations (i.e. true positive rate), and minimizes positive predictions among the negative observations (i.e. false positive rate).

In cases where the modeled output is a probability, as is the case in the models established so far, the response can be mapped to a binary outcome using a threshold value. An ROC curve plots the true positive rate against the false positive rate for all such thresholds. The area under this curve measures the closeness to a perfect classification model.

```{r message=F}
library(ROCR)
pred <- prediction(predict(model.tree, data.testing, type = "prob")[, "Y"] * data.testing$Exp_weights, data.testing$Clm_Flag)

roc.curve <- data.frame(TPR = performance(pred, "tpr", "fpr")@y.values[[1]],
                        FPR = performance(pred, "tpr", "fpr")@x.values[[1]], row.names = NULL)

ggplot(roc.curve) +
  geom_line(aes(y = TPR, x = FPR), size = 1.5) +
  labs(x = "1 - Specificity", y = "Sensitivity") +
  ggtitle(paste("Area Under ROC Curve:", round(performance(pred, measure = "auc")@y.values[[1]], 3))) +
  scale_x_continuous(breaks = seq(0, 1, 0.1)) +
  theme(legend.position = "none")
```

## Lift Charts

A useful method for comparing results across segments involves the use of lift charts, which averages the predicted or observed response (or both) for each category or band. This can be superimposed with the relative frequencies of exposures, with a focus on credible cells. Alternatively, the predicted values can be ordered and binned to force equal exposures. A two-way lift chart introduces another segment to check for interaction, while a double lift chart compares the results of two models.

```{r message=F}
library(data.table)
plot.fit.oneway <- function(df, var, exposure, response, fit){  # One-way lift chart
  modeldata <- data.frame(Weight = df[[exposure]],
                          Actual = df[[response]],
                          Fit = df[[fit]],
                          Factor = df[[var]])
  dframe <- as.data.frame(data.table(modeldata)[,.(SumWeight = sum(Weight),
                                                   SumActual = sum(Actual),
                                                   SumFit  = sum(Fit),
                                                   AverageActual = sum(Actual) / sum(Weight),
                                                   AverageFit = sum(Fit) / sum(Weight)),
                                                by = Factor])
  dframe <- dframe[order(dframe$Factor), ]
  dframe.melt <- melt(dframe [, c(1, 5, 6)], id = c("Factor"))
  weight_max <<- max(dframe$SumWeight)
  resp_max <<- max(dframe.melt$value)
  liftplot <- ggplot(dframe, aes(x = Factor, y = SumWeight, group = 1)) +
    geom_bar(stat = "identity", aes(x = Factor, y = SumWeight * (resp_max) / (weight_max)),
             col = "#317eac", fill = "#2fa4e7") +
    geom_line(data = dframe.melt, aes(x = Factor, y = value, color = variable, group = variable), size = 1.5) +
    theme(legend.position="bottom", legend.direction="horizontal", legend.title = element_blank()) +
    scale_color_manual(labels = c("Reference", "Prediction"), values = c("gray", "orange", "black")) +
    theme(axis.text.x = element_text(angle = 60, hjust = 1)) +
    labs(x = var,  y = "Average Claim Incidence", title = paste(var,"One-Way Lift Chart"))
  return(liftplot)
}

data.lift <- data.testing
data.lift$YHat <- pred.tuned2
data.lift$Clm_Flag <- as.integer(ifelse(data.lift$Clm_Flag == "Y", 1, 0)) # Recode target variable
data.lift$YHat <- as.integer(ifelse(data.lift$YHat == "Y", 1, 0))

plot.fit.oneway(data.lift, 'AgeCat', 'Exp_weights', 'Clm_Flag', 'YHat')
```

## Crunch Residuals

Other useful diagnostics involve residuals which, in the case of classification problems, are hard to interpret. One technique to overcome this challenge is to "crunch" the residuals by aggregating exposures into equally sized bins and averaging the predicted and observed responses.

```{r}

set.seed(1000)
Crunch <- function(df, fitted, target, weight, size = 50) {  # Function to compute crunch residuals
  y <- df[, target]
  yhat <- df[, fitted]
  wt <- df[, weight]
  z <- as.data.frame(cbind(y, yhat, wt))
  z <- z[sample(nrow(z)), ]  # Shuffle rows
  test <- setDT(z)[, as.list(colMeans(.SD)), by = ceiling(cumsum(z[, wt])/sum(z[, wt])*size)]  # Group data
  test$res <- test$y - test$yhat
  qplot(y = test$res, x = test$yhat, ylab = "Residual", xlab = "Fitted Value", main = "Crunched Residuals")
}

Crunch(data.lift, 'YHat', 'Clm_Flag', 'Exp_weights')
```
